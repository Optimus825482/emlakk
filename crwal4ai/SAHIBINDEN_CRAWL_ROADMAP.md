# ğŸ  Sahibinden.com Emlak Ä°lanlarÄ± Crawl4AI Yol HaritasÄ±

## ğŸ“‹ Proje Ã–zeti

**Hedef:** https://www.sahibinden.com/emlak/sakarya-hendek sayfasÄ±ndaki emlak ilanlarÄ±nÄ± crawl etmek

## ğŸš§ Ana Zorluk: Cloudflare Turnstile KorumasÄ±

Sahibinden.com, Cloudflare Turnstile bot korumasÄ± kullanÄ±yor. Bu, standart crawling yÃ¶ntemlerini engelliyor.

---

## ğŸ› ï¸ Ã‡Ã¶zÃ¼m Stratejileri (Ã–ncelik SÄ±rasÄ±na GÃ¶re)

### Strateji 1: Identity-Based Crawling (Ã–NERÄ°LEN) â­

**En gÃ¼venilir ve sÃ¼rdÃ¼rÃ¼lebilir yÃ¶ntem**

```python
# 1. Ã–nce profil oluÅŸtur (bir kez yapÄ±lÄ±r)
# Terminal'de:
# crawl4ai-setup profile

# 2. AÃ§Ä±lan tarayÄ±cÄ±da sahibinden.com'a git
# 3. Cloudflare doÄŸrulamasÄ±nÄ± manuel geÃ§
# 4. Ä°stersen giriÅŸ yap
# 5. Terminal'de 'q' bas - profil kaydedilir
```

**AvantajlarÄ±:**

- GerÃ§ek kullanÄ±cÄ± kimliÄŸi
- Cookie ve session korunur
- Cloudflare sizi tanÄ±r
- En stabil Ã§Ã¶zÃ¼m

### Strateji 2: Undetected Browser + Stealth Mode

```python
from crawl4ai import AsyncWebCrawler, BrowserConfig, CrawlerRunConfig
from crawl4ai import UndetectedAdapter
from crawl4ai.async_crawler_strategy import AsyncPlaywrightCrawlerStrategy

browser_config = BrowserConfig(
    headless=False,  # Headless=False daha iyi Ã§alÄ±ÅŸÄ±r
    enable_stealth=True,  # Stealth mode aktif
    verbose=True,
)

undetected_adapter = UndetectedAdapter()
crawler_strategy = AsyncPlaywrightCrawlerStrategy(
    browser_config=browser_config,
    browser_adapter=undetected_adapter
)
```

### Strateji 3: Magic Mode (Basit Durumlar Ä°Ã§in)

```python
run_config = CrawlerRunConfig(
    magic=True,  # Otomatik popup/consent yÃ¶netimi
    simulate_user=True,  # KullanÄ±cÄ± simÃ¼lasyonu
    override_navigator=True,  # Navigator override
)
```

### Strateji 4: CapSolver Entegrasyonu (Ãœcretli)

Cloudflare Turnstile token'Ä± almak iÃ§in CapSolver API kullanÄ±labilir.

---

## ğŸ“ Sahibinden.com HTML YapÄ±sÄ± (Tahmini Schema)

```python
# Ä°lan listesi iÃ§in CSS Schema
schema = {
    "name": "Sahibinden Emlak Ä°lanlarÄ±",
    "baseSelector": "tr.searchResultsItem",  # veya div.classified-list-item
    "fields": [
        {
            "name": "ilan_id",
            "selector": "a.classifiedTitle",
            "type": "attribute",
            "attribute": "href"
        },
        {
            "name": "baslik",
            "selector": "a.classifiedTitle",
            "type": "text"
        },
        {
            "name": "fiyat",
            "selector": "td.searchResultsPriceValue span, .classified-price-container",
            "type": "text"
        },
        {
            "name": "konum",
            "selector": "td.searchResultsLocationValue, .classified-location",
            "type": "text"
        },
        {
            "name": "tarih",
            "selector": "td.searchResultsDateValue, .classified-date",
            "type": "text"
        },
        {
            "name": "oda_sayisi",
            "selector": "td.searchResultsAttributeValue:nth-child(4)",
            "type": "text"
        },
        {
            "name": "metrekare",
            "selector": "td.searchResultsAttributeValue:nth-child(5)",
            "type": "text"
        },
        {
            "name": "resim",
            "selector": "img.searchResultsImg",
            "type": "attribute",
            "attribute": "src"
        }
    ]
}
```

---

## ğŸš€ Uygulama PlanÄ±

### Faz 1: Ortam HazÄ±rlÄ±ÄŸÄ±

```bash
# 1. Crawl4AI kurulumu
pip install crawl4ai

# 2. Playwright tarayÄ±cÄ±larÄ± kur
crawl4ai-setup

# 3. Profil oluÅŸtur (Cloudflare bypass iÃ§in)
crawl4ai-setup profile
```

### Faz 2: Temel Crawler GeliÅŸtirme

```
crwal4ai/
â”œâ”€â”€ sahibinden_crawler.py    # Ana crawler
â”œâ”€â”€ schemas/
â”‚   â””â”€â”€ emlak_schema.py      # CSS extraction schema
â”œâ”€â”€ config/
â”‚   â””â”€â”€ browser_config.py    # Browser ayarlarÄ±
â”œâ”€â”€ utils/
â”‚   â””â”€â”€ cloudflare_handler.py # CF bypass yardÄ±mcÄ±larÄ±
â””â”€â”€ output/
    â””â”€â”€ listings.json        # Ã‡Ä±ktÄ± dosyasÄ±
```

### Faz 3: Veri Ã‡Ä±karma Stratejisi

1. **JsonCssExtractionStrategy** - HÄ±zlÄ±, LLM gerektirmez
2. Sayfa yapÄ±sÄ± deÄŸiÅŸirse **LLMExtractionStrategy** backup olarak

### Faz 4: Pagination YÃ¶netimi

```python
# Sayfalama iÃ§in URL pattern
base_url = "https://www.sahibinden.com/emlak/sakarya-hendek"
# ?pagingOffset=20, ?pagingOffset=40, ...
```

---

## ğŸ“ Ã–rnek Kod YapÄ±sÄ±

```python
import asyncio
import json
from crawl4ai import (
    AsyncWebCrawler,
    BrowserConfig,
    CrawlerRunConfig,
    CacheMode
)
from crawl4ai import JsonCssExtractionStrategy

async def crawl_sahibinden():
    # Browser Config - Identity Based
    browser_config = BrowserConfig(
        headless=False,
        verbose=True,
        user_data_dir="~/.crawl4ai/profiles/sahibinden_profile",
        use_persistent_context=True,
        java_script_enabled=True,
        viewport_width=1920,
        viewport_height=1080,
    )

    # Extraction Schema
    schema = {
        "name": "Sahibinden Emlak",
        "baseSelector": "tr.searchResultsItem",
        "fields": [
            {"name": "baslik", "selector": "a.classifiedTitle", "type": "text"},
            {"name": "fiyat", "selector": "td.searchResultsPriceValue span", "type": "text"},
            {"name": "konum", "selector": "td.searchResultsLocationValue", "type": "text"},
            {"name": "link", "selector": "a.classifiedTitle", "type": "attribute", "attribute": "href"},
        ]
    }

    # Run Config
    run_config = CrawlerRunConfig(
        cache_mode=CacheMode.BYPASS,
        extraction_strategy=JsonCssExtractionStrategy(schema),
        wait_for="css:tr.searchResultsItem",
        page_timeout=60000,
        delay_before_return_html=2.0,  # Cloudflare iÃ§in bekle
    )

    async with AsyncWebCrawler(config=browser_config) as crawler:
        result = await crawler.arun(
            url="https://www.sahibinden.com/emlak/sakarya-hendek",
            config=run_config
        )

        if result.success:
            data = json.loads(result.extracted_content)
            print(f"âœ… {len(data)} ilan bulundu!")
            return data
        else:
            print(f"âŒ Hata: {result.error_message}")
            return None

if __name__ == "__main__":
    asyncio.run(crawl_sahibinden())
```

---

## âš ï¸ Ã–nemli Notlar

### Cloudflare Bypass Ä°Ã§in:

1. **headless=False** kullan (detection daha zor)
2. **GerÃ§ekÃ§i viewport** boyutlarÄ± kullan
3. **Delay** ekle (delay_before_return_html)
4. **User profile** kullan (en etkili)

### Rate Limiting:

- Ä°stekler arasÄ± **2-5 saniye** bekle
- GÃ¼nlÃ¼k **maksimum istek sayÄ±sÄ±** belirle
- **IP rotasyonu** dÃ¼ÅŸÃ¼n (proxy)

### Legal UyarÄ±:

- robots.txt'e uy
- AÅŸÄ±rÄ± yÃ¼k bindirme
- KiÅŸisel veri iÅŸleme kurallarÄ±na dikkat

---

## ğŸ”„ Sonraki AdÄ±mlar

1. [ ] Profil oluÅŸtur ve Cloudflare'Ä± geÃ§
2. [ ] HTML yapÄ±sÄ±nÄ± analiz et (gerÃ§ek selector'larÄ± bul)
3. [ ] Schema'yÄ± gÃ¼ncelle
4. [ ] Pagination ekle
5. [ ] Error handling ekle
6. [ ] Demir-gayrimenkul projesine entegre et

---

## ğŸ“š Referanslar

- [Crawl4AI Docs](https://docs.crawl4ai.com)
- [Identity Based Crawling](https://docs.crawl4ai.com/advanced/identity-based-crawling/)
- [Undetected Browser](https://docs.crawl4ai.com/advanced/undetected-browser/)
- [Hooks & Auth](https://docs.crawl4ai.com/advanced/hooks-auth/)
