"""
Sahibinden Crawler Admin Panel - Python Remix
==============================================
Crawler verilerini gÃ¶rÃ¼ntÃ¼lemek iÃ§in ayrÄ± admin arayÃ¼zÃ¼.
Ana admin panelinden baÄŸÄ±msÄ±z Ã§alÄ±ÅŸÄ±r, sadece verileri gÃ¶sterir.

KullanÄ±m:
    python app.py

Ã–zellikler:
    - Ä°lan listesi (filtreleme, arama, pagination)
    - Kategori istatistikleri
    - Yeni ilanlar
    - KaldÄ±rÄ±lan ilanlar
    - Crawler job geÃ§miÅŸi
    - Real-time dashboard
"""

from flask import Flask, render_template, jsonify, request, make_response
from db_manager import db
import json
from dotenv import load_dotenv
import os
import subprocess
import sys
from datetime import datetime, timedelta
from typing import Optional, Dict, List
import threading
import uuid

# Load environment
load_dotenv(".env")

app = Flask(__name__)
app.config["SECRET_KEY"] = "dev-secret-key-change-in-production"

# Database setup
# DATABASE_URL is expected in .env or provided by environment
DATABASE_URL = os.getenv("DATABASE_URL")
if not DATABASE_URL:
    # PUBLIC URL provided by user
    DATABASE_URL = "postgres://postgres:518518Erkan@77.42.68.4:5432/demir_db"
os.environ["DATABASE_URL"] = DATABASE_URL

# Crawler state
crawler_running = False
current_job_id = None


# ============================================================================
# HELPER FUNCTIONS
# ============================================================================


def format_price(price: int) -> str:
    """FiyatÄ± formatla: 9300000 -> 9.300.000 TL"""
    if not price:
        return "BelirtilmemiÅŸ"
    return f"{price:,.0f} TL".replace(",", ".")


def format_date(date_str: str) -> str:
    """Tarihi formatla"""
    if not date_str:
        return "-"
    try:
        dt = datetime.fromisoformat(date_str.replace("Z", "+00:00"))
        return dt.strftime("%d.%m.%Y %H:%M")
    except:
        return date_str


def get_category_display(category: str) -> str:
    """Kategori display adÄ±"""
    mapping = {"konut": "Konut", "arsa": "Arsa", "isyeri": "Ä°ÅŸyeri", "bina": "Bina"}
    return mapping.get(category, category.title())


def get_transaction_display(transaction: str) -> str:
    """Ä°ÅŸlem tipi display adÄ±"""
    mapping = {"satilik": "SatÄ±lÄ±k", "kiralik": "KiralÄ±k"}
    return mapping.get(transaction, transaction.title())


# ============================================================================
# ROUTES - PAGES
# ============================================================================


@app.route("/")
def index():
    """Ana dashboard"""
    return render_template("index.html")


@app.route("/listings")
def listings():
    """Ä°lan listesi sayfasÄ±"""
    return render_template("listings.html")


@app.route("/new-listings")
def new_listings():
    """Yeni ilanlar sayfasÄ±"""
    return render_template("new_listings.html")


@app.route("/removed-listings")
def removed_listings():
    """KaldÄ±rÄ±lan ilanlar sayfasÄ±"""
    return render_template("removed_listings.html")


@app.route("/jobs")
def jobs():
    """Crawler job geÃ§miÅŸi"""
    return render_template("jobs.html")


@app.route("/crawler")
def crawler():
    """Crawler yÃ¶netim sayfasÄ±"""
    return render_template("crawler.html")


@app.route("/map")
def map_page():
    """Hendek Emlak HaritasÄ±"""
    return render_template("map.html")


# ============================================================================
# API ENDPOINTS
# ============================================================================


@app.route("/api/crawler/status")
def api_crawler_status():
    """Crawler durumu"""
    global crawler_running, current_job_id

    try:
        status = {"running": crawler_running, "job_id": current_job_id}

        # EÄŸer job varsa detaylarÄ±nÄ± al
        if current_job_id:
            try:
                job_data = db.execute_one(
                    "SELECT * FROM mining_jobs WHERE id = %s",
                    (current_job_id,)
                )

                if job_data:
                    status["job"] = {
                        "id": job_data["id"],
                        "status": job_data["status"],
                        "created_at": format_date(job_data["created_at"].isoformat() if hasattr(job_data["created_at"], 'isoformat') else str(job_data["created_at"])),
                        "stats": job_data.get("stats", {}),
                        "progress": job_data.get("progress", {}),
                    }
            except Exception as job_error:
                print(f"Error getting job status: {job_error}")

        return jsonify({"success": True, "data": status})

    except Exception as e:
        return jsonify({"success": False, "error": str(e)}), 500


@app.route("/api/crawler/start", methods=["POST"])
def api_crawler_start():
    """Crawler baÅŸlat"""
    global crawler_running, current_job_id

    if crawler_running:
        return jsonify({"success": False, "error": "Crawler zaten Ã§alÄ±ÅŸÄ±yor"}), 400

    try:
        data = request.json or {}
        categories = data.get("categories", ["konut_satilik"])
        max_pages = data.get("max_pages", 100)
        force = data.get("force", False)
        reverse_sort = data.get("reverse_sort", False)  # NEW

        # Job ID oluÅŸtur
        job_id = str(uuid.uuid4())
        current_job_id = job_id

        # Job kaydÄ± oluÅŸtur
        job_sql = """
            INSERT INTO mining_jobs (id, job_type, status, config, stats, progress)
            VALUES (%s, %s, %s, %s, %s, %s)
        """
        db.execute_query(
            job_sql, 
            (
                job_id, 
                "manual_crawler", 
                "running", 
                json.dumps({
                    "categories": categories,
                    "max_pages": max_pages,
                    "force": force,
                    "reverse_sort": reverse_sort,
                }),
                json.dumps({}),
                json.dumps({"current": 0, "total": 0, "percentage": 0})
            ),
            fetch=False
        )

        # Crawler'Ä± background thread'de baÅŸlat
        def run_crawler():
            global crawler_running
            crawler_running = True

            try:
                # Python script yolu (UC Batch Crawler)
                script_path = os.path.join(os.path.dirname(__file__), "sahibinden_uc_batch_supabase.py")

                # Komut hazÄ±rla
                cmd = [
                    sys.executable,
                    script_path,
                    "--categories",
                    *categories,
                    "--max-pages",
                    str(max_pages),
                    "--job-id",
                    job_id,
                ]

                if force:
                    print("âš¡ Force mode detected")
                    cmd.append("--force")
                    # Force modunda otomatik sync de yapalÄ±m mÄ±?
                    # KullanÄ±cÄ± "Fix it" dediÄŸinde muhtemelen bunu istiyor
                    # Ama ÅŸimdilik ayrÄ± tutalÄ±m veya force ise sync de true yapalÄ±m
                    # cmd.append('--sync') <--- KullanÄ±cÄ± seÃ§meli olmalÄ±

                if reverse_sort:
                    print("ğŸ”„ Reverse sort detected")
                    cmd.append("--reverse-sort")

                # Sync parametresi (API'den gelmeli)
                # KullanÄ±cÄ± "Ekle" dediÄŸi iÃ§in bu Ã¶zellik aÃ§Ä±k olmalÄ±.
                if data.get("sync", False):
                    print("ğŸ—‘ï¸ Sync mode detected (implying Turbo)")
                    cmd.append("--sync")
                    # Sync modunda otomatik turbo:
                    cmd.append("--turbo")

                # Manual turbo
                if data.get("turbo", False):
                    cmd.append("--turbo")

                # Crawler'Ä± Ã§alÄ±ÅŸtÄ±r
                print(f"Executing: {' '.join(cmd)}")
                script_dir = os.path.dirname(script_path)
                log_file = os.path.join(script_dir, "crawler_debug.log")
                
                # Windows'ta detached process (penceresiz)
                creationflags = 0
                if sys.platform == "win32":
                    creationflags = 0x00000008  # DETACHED_PROCESS

                with open(log_file, "w", encoding="utf-8") as f:
                    result = subprocess.run(
                        cmd,
                        stdout=f,
                        stderr=subprocess.STDOUT,
                        text=True,
                        timeout=3600,  # 1 saat timeout
                        cwd=script_dir,  # Ã‡alÄ±ÅŸma dizinini script'in olduÄŸu yer yap
                        creationflags=creationflags,
                    )

                # Debug logu oku (sonra gÃ¼ncellemek iÃ§in)
                try:
                    with open(log_file, "r", encoding="utf-8") as f:
                        debug_log = f.read()
                except:
                    debug_log = "Log file read error"

                # Job'u gÃ¼ncelle
                if result.returncode == 0:
                    supabase.table("mining_jobs").update(
                        {
                            "status": "completed",
                            "error": debug_log[-5000:] if debug_log else "No output",
                        }
                    ).eq("id", job_id).execute()
                else:
                    supabase.table("mining_jobs").update(
                        {"status": "failed", "error": debug_log[-5000:]}
                    ).eq("id", job_id).execute()

            except subprocess.TimeoutExpired:
                supabase.table("mining_jobs").update(
                    {"status": "failed", "error": "Timeout (1 saat)"}
                ).eq("id", job_id).execute()

            except Exception as e:
                supabase.table("mining_jobs").update(
                    {"status": "failed", "error": str(e)[:500]}
                ).eq("id", job_id).execute()

            finally:
                crawler_running = False

        # Thread baÅŸlat
        thread = threading.Thread(target=run_crawler, daemon=True)
        thread.start()

        return jsonify(
            {"success": True, "message": "Crawler baÅŸlatÄ±ldÄ±", "job_id": job_id}
        )

    except Exception as e:
        crawler_running = False
        return jsonify({"success": False, "error": str(e)}), 500


@app.route("/api/crawler/stop", methods=["POST"])
def api_crawler_stop():
    """Crawler durdur (ÅŸu an desteklenmiyor)"""
    return jsonify(
        {"success": False, "error": "Crawler durdurulamaz. TamamlanmasÄ±nÄ± bekleyin."}
    ), 400


@app.route("/api/crawler/start-parallel", methods=["POST"])
def api_crawler_start_parallel():
    """Paralel Crawler baÅŸlat (2 Chrome worker)"""
    global crawler_running, current_job_id

    if crawler_running:
        return jsonify({"success": False, "error": "Crawler zaten Ã§alÄ±ÅŸÄ±yor"}), 400

    try:
        data = request.json or {}
        categories = data.get("categories", ["konut_satilik"])
        max_pages = data.get("max_pages", 100)
        turbo = data.get("turbo", False)
        sync = data.get("sync", False)

        # Job ID oluÅŸtur
        job_id = str(uuid.uuid4())
        current_job_id = job_id

        # Job kaydÄ± oluÅŸtur
        job_sql = """
            INSERT INTO mining_jobs (id, job_type, status, config, stats, progress)
            VALUES (%s, %s, %s, %s, %s, %s)
        """
        db.execute_query(
            job_sql,
            (
                job_id,
                "parallel_crawler",
                "running",
                json.dumps({
                    "categories": categories,
                    "max_pages": max_pages,
                    "workers": 2,
                    "turbo": turbo,
                    "sync": sync,
                }),
                json.dumps({}),
                json.dumps({"current": 0, "total": len(categories), "percentage": 0})
            ),
            fetch=False
        )

        # Paralel Crawler'Ä± background thread'de baÅŸlat
        def run_parallel_crawler():
            global crawler_running
            crawler_running = True

            try:
                # Paralel crawler script yolu
                script_path = os.path.join(os.path.dirname(__file__), "parallel_crawler.py")

                # Komut hazÄ±rla
                cmd = [
                    sys.executable,
                    script_path,
                    "--categories",
                    *categories,
                    "--max-pages",
                    str(max_pages),
                    "--job-id",
                    job_id,
                ]

                if turbo:
                    cmd.append("--turbo")

                if sync:
                    cmd.append("--sync")

                # Crawler'Ä± Ã§alÄ±ÅŸtÄ±r
                print(f"Executing Parallel Crawler: {' '.join(cmd)}")
                script_dir = os.path.dirname(script_path)
                log_file = os.path.join(script_dir, "parallel_crawler_debug.log")
                
                # Windows'ta detached process (penceresiz)
                creationflags = 0
                if sys.platform == "win32":
                    creationflags = 0x00000008  # DETACHED_PROCESS
                
                with open(log_file, "w", encoding="utf-8") as f:
                    result = subprocess.run(
                        cmd,
                        stdout=f,
                        stderr=subprocess.STDOUT,
                        text=True,
                        timeout=3600,  # 1 saat timeout
                        cwd=script_dir,
                        creationflags=creationflags,
                    )

                # DEBUG: Capture output
                try:
                    with open(log_file, "r", encoding="utf-8") as f:
                        debug_log = f.read()
                except:
                    debug_log = "Log read error"

                # Job'u gÃ¼ncelle
                if result.returncode == 0:
                    supabase.table("mining_jobs").update(
                        {
                            "status": "completed",
                            "error": debug_log[-5000:] if debug_log else "No output",
                        }
                    ).eq("id", job_id).execute()
                else:
                    supabase.table("mining_jobs").update(
                        {"status": "failed", "error": debug_log[-5000:]}
                    ).eq("id", job_id).execute()

            except subprocess.TimeoutExpired:
                db.execute_query(
                    "UPDATE mining_jobs SET status = %s, error = %s WHERE id = %s",
                    ("failed", "Timeout (1 saat)", job_id),
                    fetch=False
                )

            except Exception as e:
                db.execute_query(
                    "UPDATE mining_jobs SET status = %s, error = %s WHERE id = %s",
                    ("failed", str(e)[:500], job_id),
                    fetch=False
                )

            finally:
                crawler_running = False

        # Thread baÅŸlat
        thread = threading.Thread(target=run_parallel_crawler, daemon=True)
        thread.start()

        return jsonify(
            {
                "success": True,
                "message": "Paralel Crawler baÅŸlatÄ±ldÄ± (2 Worker)",
                "job_id": job_id,
            }
        )

    except Exception as e:
        crawler_running = False
        return jsonify({"success": False, "error": str(e)}), 500


@app.route("/api/category-counts")
def api_category_counts():
    """Kategori bazÄ±nda ilan sayÄ±larÄ± (veritabanÄ±ndan)"""
    try:
        counts = {}

        # TÃ¼m kategori kombinasyonlarÄ±
        categories = [
            ("konut", "satilik", "konut_satilik"),
            ("konut", "kiralik", "konut_kiralik"),
            ("arsa", "satilik", "arsa_satilik"),
            ("isyeri", "satilik", "isyeri_satilik"),
            ("isyeri", "kiralik", "isyeri_kiralik"),
            ("bina", "satilik", "bina_satilik"),
            ("bina", "kiralik", "bina_kiralik"),
        ]

        for category, transaction, key in categories:
            res = db.execute_one(
                "SELECT COUNT(*) as count FROM sahibinden_liste WHERE category = %s AND transaction = %s",
                (category, transaction)
            )
            counts[key] = res["count"] if res else 0

        return jsonify({"success": True, "data": counts})

    except Exception as e:
        return jsonify({"success": False, "error": str(e)}), 500


@app.route("/api/dashboard")
def api_dashboard():
    """Dashboard Ã¶zet verileri - YÃ¼ksek PerformanslÄ± ve DoÄŸru Versiyon"""
    try:
        # Pazar verilerinin tarih filtresi
        days = int(request.args.get("days", 1))
        # ISO formatÄ±nda tarih sÄ±nÄ±rÄ± (Ã¶rneÄŸin 2026-01-19T00:00:00)
        days_ago = (datetime.now() - timedelta(days=days)).isoformat()

        # 1. Toplam Ä°lan SayÄ±sÄ±
        total_res = db.execute_one("SELECT COUNT(*) as count FROM sahibinden_liste")
        total_count = total_res["count"] if total_res else 0

        # 2. Yeni Ä°lanlar
        new_res = db.execute_one(
            "SELECT COUNT(*) as count FROM new_listings WHERE created_at >= %s",
            (days_ago,)
        )
        new_listings_count = new_res["count"] if new_res else 0

        # 3. KaldÄ±rÄ±lan Ä°lanlar
        rem_res = db.execute_one(
            "SELECT COUNT(*) as count FROM removed_listings WHERE removed_at >= %s",
            (days_ago,)
        )
        removed_listings_count = rem_res["count"] if rem_res else 0

        # 4. Kategori DaÄŸÄ±lÄ±mÄ± (Optimized SQL-like counts)
        # Not: Bu kÄ±sÄ±m kategori bazlÄ± Ã¶zetleri toplar
        categories = ["konut", "arsa", "isyeri", "bina"]
        category_data = {}

        for cat in categories:
            # Mevcut VeritabanÄ± SayÄ±sÄ±
            sat_count = db.execute_one(
                "SELECT COUNT(*) as count FROM sahibinden_liste WHERE category = %s AND transaction = %s",
                (cat, "satilik")
            )["count"]
            kir_count = db.execute_one(
                "SELECT COUNT(*) as count FROM sahibinden_liste WHERE category = %s AND transaction = %s",
                (cat, "kiralik")
            )["count"]

            # Yeni Bulunanlar
            new_sat = db.execute_one(
                "SELECT COUNT(*) as count FROM new_listings WHERE category = %s AND transaction = %s AND created_at >= %s",
                (cat, "satilik", days_ago)
            )["count"]
            new_kir = db.execute_one(
                "SELECT COUNT(*) as count FROM new_listings WHERE category = %s AND transaction = %s AND created_at >= %s",
                (cat, "kiralik", days_ago)
            )["count"]

            category_data[cat] = {
                "satilik": sat_count,
                "kiralik": kir_count,
                "new_satilik": new_sat,
                "new_kiralik": new_kir,
            }

        # 5. Son Ä°ÅŸlem (Mining Jobs)
        last_job_data = db.execute_one(
            "SELECT * FROM mining_jobs ORDER BY created_at DESC LIMIT 1"
        )

        last_job = None
        if last_job_data:
            job = last_job_data
            last_job = {
                "id": job.get("id"),
                "status": job.get("status"),
                "created_at": format_date(job.get("created_at").isoformat() if hasattr(job.get("created_at"), 'isoformat') else str(job.get("created_at"))),
                "stats": job.get("stats") or {},
                "config": job.get("config") or {},
            }

        return jsonify(
            {
                "success": True,
                "data": {
                    "total_listings": total_count,
                    "new_listings": new_listings_count,
                    "removed_listings": removed_listings_count,
                    "categories": category_data,
                    "last_job": last_job,
                    "days": days,
                },
            }
        )

    except Exception as e:
        app.logger.error(f"Dashboard API Error: {str(e)}")
        return jsonify({"success": False, "error": str(e)}), 500


@app.route("/api/listings")
def api_listings():
    """Ä°lan listesi (pagination, filtreleme)"""
    try:
        # Query params
        page = int(request.args.get("page", 1))
        per_page = int(request.args.get("per_page", 20))
        category = request.args.get("category")
        transaction = request.args.get("transaction")
        search = request.args.get("search")

        # Build SQL query with filters and pagination
        sql = "SELECT *, COUNT(*) OVER() as full_count FROM sahibinden_liste WHERE 1=1"
        params = []

        if category:
            sql += " AND category = %s"
            params.append(category)
        if transaction:
            sql += " AND transaction = %s"
            params.append(transaction)
        if search:
            sql += " AND baslik ILIKE %s"
            params.append(f"%{search}%")

        sql += " ORDER BY crawled_at DESC LIMIT %s OFFSET %s"
        params.extend([per_page, (page - 1) * per_page])

        # Execute
        results = db.execute_query(sql, params)
        total_count = results[0]["full_count"] if results else 0

        # Format
        listings = []
        for item in (results or []):
            listings.append(
                {
                    "id": item["id"],
                    "baslik": item["baslik"],
                    "fiyat": format_price(item["fiyat"]),
                    "fiyat_raw": item["fiyat"],
                    "konum": item["konum"],
                    "category": get_category_display(item["category"]),
                    "transaction": get_transaction_display(item["transaction"]),
                    "tarih": item["tarih"],
                    "crawled_at": format_date(item["crawled_at"].isoformat() if hasattr(item["crawled_at"], 'isoformat') else str(item["crawled_at"])),
                    "link": item["link"],
                    "resim": item["resim"],
                }
            )

        return jsonify(
            {
                "success": True,
                "data": listings,
                "pagination": {
                    "page": page,
                    "per_page": per_page,
                    "total": total_count,
                    "total_pages": (total_count + per_page - 1) // per_page,
                },
            }
        )

    except Exception as e:
        return jsonify({"success": False, "error": str(e)}), 500


@app.route("/api/new-listings")
def api_new_listings():
    """Yeni ilanlar listesi"""
    try:
        page = int(request.args.get("page", 1))
        per_page = int(request.args.get("per_page", 20))
        days = int(request.args.get("days", 7))

        # Son X gÃ¼n
        date_threshold = (datetime.now() - timedelta(days=days)).isoformat()

        # Query
        sql = "SELECT *, COUNT(*) OVER() as full_count FROM new_listings WHERE first_seen_at >= %s ORDER BY first_seen_at DESC LIMIT %s OFFSET %s"
        params = [date_threshold, per_page, (page - 1) * per_page]

        results = db.execute_query(sql, params)
        total_count = results[0]["full_count"] if results else 0

        # Format
        listings = []
        for item in (results or []):
            listings.append(
                {
                    "listing_id": item["listing_id"],
                    "baslik": item["baslik"],
                    "fiyat": format_price(item["fiyat"]),
                    "konum": item["konum"],
                    "category": get_category_display(item["category"]),
                    "transaction": get_transaction_display(item["transaction"]),
                    "first_seen_at": format_date(item["first_seen_at"].isoformat() if hasattr(item["first_seen_at"], 'isoformat') else str(item["first_seen_at"])),
                    "link": item["link"],
                    "resim": item["resim"],
                }
            )

        return jsonify(
            {
                "success": True,
                "data": listings,
                "pagination": {
                    "page": page,
                    "per_page": per_page,
                    "total": total_count,
                    "total_pages": (total_count + per_page - 1) // per_page,
                },
            }
        )

    except Exception as e:
        return jsonify({"success": False, "error": str(e)}), 500


@app.route("/api/removed-listings")
def api_removed_listings():
    """KaldÄ±rÄ±lan ilanlar listesi"""
    try:
        page = int(request.args.get("page", 1))
        per_page = int(request.args.get("per_page", 20))
        days = int(request.args.get("days", 30))

        # Son X gÃ¼n
        date_threshold = (datetime.now() - timedelta(days=days)).isoformat()

        # Query
        sql = "SELECT *, COUNT(*) OVER() as full_count FROM removed_listings WHERE removed_at >= %s ORDER BY removed_at DESC LIMIT %s OFFSET %s"
        params = [date_threshold, per_page, (page - 1) * per_page]

        results = db.execute_query(sql, params)
        total_count = results[0]["full_count"] if results else 0

        # Format
        listings = []
        for item in (results or []):
            listings.append(
                {
                    "listing_id": item["listing_id"],
                    "baslik": item["baslik"],
                    "fiyat": format_price(item["last_price"]),
                    "konum": item["konum"],
                    "category": get_category_display(item["category"]),
                    "transaction": get_transaction_display(item["transaction"]),
                    "removed_at": format_date(item["removed_at"].isoformat() if hasattr(item["removed_at"], 'isoformat') else str(item["removed_at"])),
                    "days_active": item.get("days_active"),
                    "price_changes": item.get("price_changes", 0),
                    "link": item["link"],
                    "resim": item["resim"],
                }
            )

        return jsonify(
            {
                "success": True,
                "data": listings,
                "pagination": {
                    "page": page,
                    "per_page": per_page,
                    "total": total_count,
                    "total_pages": (total_count + per_page - 1) // per_page,
                },
            }
        )

    except Exception as e:
        return jsonify({"success": False, "error": str(e)}), 500


@app.route("/api/maintenance/run", methods=["POST"])
def run_maintenance():
    """VeritabanÄ± bakÄ±m ve temizlik iÅŸlemini tetikler"""
    try:
        # Supabase RPC Ã§aÄŸrÄ±sÄ±
        result = supabase.rpc("admin_maintenance_cleanup", {}).execute()

        # String olarak dÃ¶necek (SQL fonksiyonu TEXT dÃ¶ndÃ¼rÃ¼yor)
        data = result.data
        if isinstance(data, str):
            import json

            try:
                data = json.loads(data)
            except:
                pass  # Zaten obje ise veya parse edilemezse

        return jsonify(data)
    except Exception as e:
        print(f"Maintenance Error: {e}")
        return jsonify(
            {
                "success": False,
                "message": f"BakÄ±m iÅŸlemi baÅŸarÄ±sÄ±z: {str(e)}",
                "duplicates_removed": 0,
                "nulls_removed": 0,
            }
        ), 500


@app.route("/api/category-stats")
def api_category_stats():
    """Kategori istatistikleri - GerÃ§ek ZamanlÄ± VeritabanÄ± SayÄ±mlÄ±"""
    try:
        # Kategori istatistiklerini Ã§ek
        result = (
            supabase.table("category_stats")
            .select("*")
            .order("last_checked_at", desc=True)
            .execute()
        )

        stats = []
        for item in result.data:
            cat = item["category"]
            trans = item["transaction"]

            # VeritabanÄ±ndaki GERÃ‡EK sayÄ±yÄ± anlÄ±k olarak say (TutarsÄ±zlÄ±ÄŸÄ± Ã¶nle)
            real_db_res = (
                supabase.table("sahibinden_liste")
                .select("id", count="exact")
                .eq("category", cat)
                .eq("transaction", trans)
                .limit(1)
                .execute()
            )

            real_db_count = real_db_res.count or 0

            # FarkÄ± yeniden hesapla
            diff = item["sahibinden_count"] - real_db_count

            # Durumu yeniden belirle
            status = "synced"
            if diff > 0:
                status = "new"
            elif diff < 0:
                status = "removed"

            stats.append(
                {
                    "category": get_category_display(cat),
                    "transaction": get_transaction_display(trans),
                    "sahibinden_count": item["sahibinden_count"],
                    "database_count": real_db_count,
                    "diff": diff,
                    "status": status,
                    "last_checked_at": format_date(item["last_checked_at"]),
                }
            )

        return jsonify({"success": True, "data": stats})

    except Exception as e:
        app.logger.error(f"Category Stats API Error: {str(e)}")
        return jsonify({"success": False, "error": str(e)}), 500


@app.route("/api/jobs")
def api_jobs():
    """Crawler job geÃ§miÅŸi"""
    try:
        page = int(request.args.get("page", 1))
        per_page = int(request.args.get("per_page", 10))

        start = (page - 1) * per_page
        end = start + per_page - 1

        result = (
            supabase.table("mining_jobs")
            .select("*", count="exact")
            .order("created_at", desc=True)
            .range(start, end)
            .execute()
        )

        jobs = []
        for item in result.data:
            stats = item.get("stats") or {}
            jobs.append(
                {
                    "id": item["id"],
                    "status": item["status"],
                    "created_at": format_date(item["created_at"]),
                    "updated_at": format_date(item.get("updated_at")),
                    "total_listings": stats.get("total_listings", 0) if stats else 0,
                    "new_listings": stats.get("new_listings", 0) if stats else 0,
                    "updated_listings": stats.get("updated_listings", 0)
                    if stats
                    else 0,
                    "removed_listings": stats.get("removed_listings", 0)
                    if stats
                    else 0,
                    "categories_completed": stats.get("categories_completed", [])
                    if stats
                    else [],
                }
            )

        return jsonify(
            {
                "success": True,
                "data": jobs,
                "pagination": {
                    "page": page,
                    "per_page": per_page,
                    "total": result.count,
                    "total_pages": (result.count + per_page - 1) // per_page,
                },
            }
        )

    except Exception as e:
        return jsonify({"success": False, "error": str(e)}), 500


@app.route("/api/map/neighborhoods")
def api_map_neighborhoods():
    """Mahalle bazlÄ± ilan istatistikleri"""
    try:
        # TÃ¼m ilanlarÄ± Ã§ek ve mahalle bazlÄ± grupla
        result = (
            supabase.table("sahibinden_liste")
            .select("konum, category, transaction, fiyat")
            .execute()
        )

        # Mahalle bazlÄ± gruplama
        neighborhoods = {}
        
        for item in result.data:
            konum = item.get("konum", "")
            if not konum:
                continue
                
            # Mahalle adÄ±nÄ± parse et (Ã¶rn: "MerkezYeni Mah." -> "Yeni")
            mahalle = konum.replace("Merkez", "").replace("KÃ¶yler", "").replace(" Mah.", "").replace(" Mh.", "").strip()
            
            if not mahalle:
                continue
                
            if mahalle not in neighborhoods:
                neighborhoods[mahalle] = {
                    "name": mahalle,
                    "total": 0,
                    "satilik": 0,
                    "kiralik": 0,
                    "konut": 0,
                    "arsa": 0,
                    "isyeri": 0,
                    "bina": 0,
                    "avg_price": 0,
                    "min_price": float('inf'),
                    "max_price": 0,
                    "prices": []
                }
            
            neighborhoods[mahalle]["total"] += 1
            
            # Transaction type
            if item.get("transaction") == "satilik":
                neighborhoods[mahalle]["satilik"] += 1
            elif item.get("transaction") == "kiralik":
                neighborhoods[mahalle]["kiralik"] += 1
            
            # Category
            category = item.get("category", "")
            if category in neighborhoods[mahalle]:
                neighborhoods[mahalle][category] += 1
            
            # Price
            fiyat = item.get("fiyat", 0)
            if fiyat and fiyat > 0:
                neighborhoods[mahalle]["prices"].append(fiyat)
                neighborhoods[mahalle]["min_price"] = min(neighborhoods[mahalle]["min_price"], fiyat)
                neighborhoods[mahalle]["max_price"] = max(neighborhoods[mahalle]["max_price"], fiyat)
        
        # Ortalama fiyat hesapla
        for mahalle in neighborhoods.values():
            if mahalle["prices"]:
                mahalle["avg_price"] = int(sum(mahalle["prices"]) / len(mahalle["prices"]))
            else:
                mahalle["avg_price"] = 0
            
            if mahalle["min_price"] == float('inf'):
                mahalle["min_price"] = 0
            
            # prices listesini kaldÄ±r (gereksiz)
            del mahalle["prices"]
        
        # Liste olarak dÃ¶ndÃ¼r
        neighborhood_list = sorted(neighborhoods.values(), key=lambda x: x["total"], reverse=True)
        
        return jsonify({
            "success": True,
            "data": neighborhood_list,
            "total_neighborhoods": len(neighborhood_list)
        })
        
    except Exception as e:
        app.logger.error(f"Map Neighborhoods API Error: {str(e)}")
        return jsonify({"success": False, "error": str(e)}), 500


@app.route("/api/map/listings")
def api_map_listings():
    """Harita iÃ§in ilan listesi (mahalle filtrelemeli)"""
    try:
        neighborhood = request.args.get("neighborhood")
        category = request.args.get("category")
        transaction = request.args.get("transaction")
        
        # Base query
        query = supabase.table("sahibinden_liste").select("*")
        
        # Filters
        if neighborhood:
            # Mahalle filtresi (konum iÃ§inde arama)
            query = query.ilike("konum", f"%{neighborhood}%")
        
        if category:
            query = query.eq("category", category)
        
        if transaction:
            query = query.eq("transaction", transaction)
        
        # Execute (limit 100)
        result = query.order("crawled_at", desc=True).limit(100).execute()
        
        # Format
        listings = []
        for item in result.data:
            konum = item.get("konum", "")
            mahalle = konum.replace("Merkez", "").replace("KÃ¶yler", "").replace(" Mah.", "").replace(" Mh.", "").strip()
            
            listings.append({
                "id": item["id"],
                "baslik": item["baslik"],
                "fiyat": item["fiyat"],
                "fiyat_formatted": format_price(item["fiyat"]),
                "konum": konum,
                "mahalle": mahalle,
                "category": item["category"],
                "category_display": get_category_display(item["category"]),
                "transaction": item["transaction"],
                "transaction_display": get_transaction_display(item["transaction"]),
                "link": item["link"],
                "resim": item["resim"],
                "crawled_at": format_date(item["crawled_at"])
            })
        
        return jsonify({
            "success": True,
            "data": listings,
            "total": len(listings)
        })
        
    except Exception as e:
        app.logger.error(f"Map Listings API Error: {str(e)}")
        return jsonify({"success": False, "error": str(e)}), 500


# ============================================================================
# RUN
# ============================================================================

if __name__ == "__main__":
    print("=" * 60)
    print("ğŸš€ Sahibinden Crawler Admin Panel")
    print("=" * 60)
    print(f"ğŸ“ URL: http://localhost:5001")
    print(f"ğŸ“Š Dashboard: http://localhost:5001/")
    print(f"ğŸ“‹ Ä°lanlar: http://localhost:5001/listings")
    print(f"ğŸ†• Yeni Ä°lanlar: http://localhost:5001/new-listings")
    print(f"ğŸ“¤ KaldÄ±rÄ±lan: http://localhost:5001/removed-listings")
    print(f"ğŸ“ˆ Ä°statistikler: http://localhost:5001/stats")
    print(f"ğŸ”§ Jobs: http://localhost:5001/jobs")
    print("=" * 60)

    app.run(host="0.0.0.0", port=5001, debug=True)
