"""
Sahibinden Toplu Crawler
=========================
TÃ¼m Hendek ilanlarÄ±nÄ± kategorilere gÃ¶re toplu Ã§eker.

KullanÄ±m:
   python batch_crawler.py

Ã–zellikler:
- TÃ¼m kategorileri sÄ±rayla tarar
- Rate limiting (sayfa arasÄ± bekleme)
- Progress tracking
- Checkpoint/resume desteÄŸi
- JSON Ã§Ä±ktÄ±
"""

import asyncio
import json
import os
from datetime import datetime
from typing import Optional
from sahibinden_crawl4ai import SahibindenCrawl4AI

# Hendek kategorileri ve URL'leri
HENDEK_CATEGORIES = {
    "konut_satilik": {
        "url": "https://www.sahibinden.com/satilik/sakarya-hendek",
        "category": "konut",
        "transaction": "satilik",
    },
    "konut_kiralik": {
        "url": "https://www.sahibinden.com/kiralik/sakarya-hendek",
        "category": "konut",
        "transaction": "kiralik",
    },
    "isyeri_satilik": {
        "url": "https://www.sahibinden.com/satilik-isyeri/sakarya-hendek",
        "category": "isyeri",
        "transaction": "satilik",
    },
    "isyeri_kiralik": {
        "url": "https://www.sahibinden.com/kiralik-isyeri/sakarya-hendek",
        "category": "isyeri",
        "transaction": "kiralik",
    },
    "arsa_satilik": {
        "url": "https://www.sahibinden.com/satilik-arsa/sakarya-hendek",
        "category": "arsa",
        "transaction": "satilik",
    },
    "arsa_kiralik": {
        "url": "https://www.sahibinden.com/kiralik-arsa/sakarya-hendek",
        "category": "arsa",
        "transaction": "kiralik",
    },
    "bina_satilik": {
        "url": "https://www.sahibinden.com/satilik-bina/sakarya-hendek",
        "category": "bina",
        "transaction": "satilik",
    },
    "bina_kiralik": {
        "url": "https://www.sahibinden.com/kiralik-bina/sakarya-hendek",
        "category": "bina",
        "transaction": "kiralik",
    },
}

# Ayarlar
PAGE_DELAY = 5  # Sayfalar arasÄ± bekleme (saniye)
CATEGORY_DELAY = 10  # Kategoriler arasÄ± bekleme (saniye)
MAX_PAGES_PER_CATEGORY = 100  # Kategori baÅŸÄ±na max sayfa (20 ilan/sayfa = 2000 ilan)
CHECKPOINT_FILE = "batch_checkpoint.json"
OUTPUT_FILE = "hendek_tum_ilanlar.json"


class BatchCrawler:
    def __init__(self, headless: bool = False):
        self.crawler = SahibindenCrawl4AI()
        self.headless = headless
        self.all_listings = []
        self.stats = {
            "started_at": None,
            "completed_at": None,
            "categories_completed": [],
            "total_listings": 0,
            "total_pages": 0,
            "errors": [],
        }
        self.checkpoint = self.load_checkpoint()

    def load_checkpoint(self) -> dict:
        """Checkpoint dosyasÄ±nÄ± yÃ¼kle"""
        if os.path.exists(CHECKPOINT_FILE):
            try:
                with open(CHECKPOINT_FILE, "r", encoding="utf-8") as f:
                    return json.load(f)
            except:
                pass
        return {"completed_categories": [], "listings": []}

    def save_checkpoint(self):
        """Checkpoint kaydet"""
        checkpoint = {
            "completed_categories": self.stats["categories_completed"],
            "listings": self.all_listings,
            "stats": self.stats,
            "saved_at": datetime.now().isoformat(),
        }
        with open(CHECKPOINT_FILE, "w", encoding="utf-8") as f:
            json.dump(checkpoint, f, ensure_ascii=False, indent=2)
        print(f"   ðŸ’¾ Checkpoint kaydedildi ({len(self.all_listings)} ilan)")

    def save_output(self):
        """Final Ã§Ä±ktÄ±yÄ± kaydet"""
        output = {
            "crawled_at": datetime.now().isoformat(),
            "stats": self.stats,
            "total_listings": len(self.all_listings),
            "listings": self.all_listings,
        }
        with open(OUTPUT_FILE, "w", encoding="utf-8") as f:
            json.dump(output, f, ensure_ascii=False, indent=2)
        print(f"\nðŸ’¾ TÃ¼m ilanlar kaydedildi: {OUTPUT_FILE}")

    async def crawl_category(self, key: str, config: dict) -> list:
        """Tek kategoriyi crawl et"""
        url = config["url"]
        category = config["category"]
        transaction = config["transaction"]

        print(f"\n{'='*60}")
        print(f"ðŸ“‚ Kategori: {key}")
        print(f"   URL: {url}")
        print(f"   Tip: {category} / {transaction}")
        print(f"{'='*60}")

        category_listings = []
        page = 0

        while page < MAX_PAGES_PER_CATEGORY:
            page_url = url if page == 0 else f"{url}?pagingOffset={page * 20}"
            print(f"\nðŸ“„ Sayfa {page + 1} taranÄ±yor...")

            try:
                result = await self.crawler.navigate_with_bypass(page_url)

                if not result["success"]:
                    print(f"   âŒ Sayfa yÃ¼klenemedi: {result.get('error', 'Bilinmeyen hata')}")
                    self.stats["errors"].append({
                        "category": key,
                        "page": page + 1,
                        "error": result.get("error"),
                    })
                    break

                listings = await self.crawler.extract_listings(result["html"])

                if not listings:
                    print(f"   â„¹ï¸ Bu sayfada ilan yok, kategori tamamlandÄ±")
                    break

                # Kategori bilgisi ekle
                for listing in listings:
                    listing["category"] = category
                    listing["transaction"] = transaction
                    listing["crawled_at"] = datetime.now().isoformat()

                category_listings.extend(listings)
                self.stats["total_pages"] += 1

                print(f"   âœ… {len(listings)} ilan bulundu (Kategori toplam: {len(category_listings)})")

                # Sonraki sayfa iÃ§in bekle
                if page < MAX_PAGES_PER_CATEGORY - 1 and listings:
                    print(f"   â³ {PAGE_DELAY} saniye bekleniyor...")
                    await asyncio.sleep(PAGE_DELAY)

                page += 1

            except Exception as e:
                print(f"   âŒ Hata: {e}")
                self.stats["errors"].append({
                    "category": key,
                    "page": page + 1,
                    "error": str(e),
                })
                break

        return category_listings

    async def run(self, categories: Optional[list] = None):
        """Toplu taramayÄ± baÅŸlat"""
        print("=" * 60)
        print("ðŸš€ SAHÄ°BÄ°NDEN TOPLU CRAWLER")
        print("   Hendek TÃ¼m Ä°lanlar")
        print("=" * 60)

        self.stats["started_at"] = datetime.now().isoformat()

        # Checkpoint'ten devam et
        if self.checkpoint.get("listings"):
            self.all_listings = self.checkpoint["listings"]
            print(f"\nðŸ“¥ Checkpoint'ten {len(self.all_listings)} ilan yÃ¼klendi")

        # Crawler baÅŸlat - headless=False Ã¶nerilir (Cloudflare bypass iÃ§in)
        await self.crawler.setup(headless=self.headless)

        try:
            # Kategorileri belirle
            cats_to_crawl = categories or list(HENDEK_CATEGORIES.keys())
            completed = self.checkpoint.get("completed_categories", [])

            for key in cats_to_crawl:
                if key in completed:
                    print(f"\nâ­ï¸ {key} zaten tamamlanmÄ±ÅŸ, atlanÄ±yor...")
                    continue

                if key not in HENDEK_CATEGORIES:
                    print(f"\nâš ï¸ Bilinmeyen kategori: {key}")
                    continue

                config = HENDEK_CATEGORIES[key]
                listings = await self.crawl_category(key, config)

                self.all_listings.extend(listings)
                self.stats["categories_completed"].append(key)
                self.stats["total_listings"] = len(self.all_listings)

                # Checkpoint kaydet
                self.save_checkpoint()

                # Kategoriler arasÄ± bekleme
                remaining = [k for k in cats_to_crawl if k not in self.stats["categories_completed"]]
                if remaining:
                    print(f"\nâ³ Sonraki kategori iÃ§in {CATEGORY_DELAY} saniye bekleniyor...")
                    await asyncio.sleep(CATEGORY_DELAY)

        except KeyboardInterrupt:
            print("\n\nâ¸ï¸ KullanÄ±cÄ± tarafÄ±ndan durduruldu")
            self.save_checkpoint()

        except Exception as e:
            print(f"\nâŒ Kritik hata: {e}")
            self.save_checkpoint()
            raise

        finally:
            await self.crawler.close()

        self.stats["completed_at"] = datetime.now().isoformat()
        self.save_output()

        # Ã–zet
        print("\n" + "=" * 60)
        print("ðŸ“Š Ã–ZET")
        print("=" * 60)
        print(f"   Toplam ilan: {len(self.all_listings)}")
        print(f"   Toplam sayfa: {self.stats['total_pages']}")
        print(f"   Tamamlanan kategoriler: {len(self.stats['categories_completed'])}")
        print(f"   Hatalar: {len(self.stats['errors'])}")

        # Kategori bazlÄ± daÄŸÄ±lÄ±m
        print("\nðŸ“‚ Kategori DaÄŸÄ±lÄ±mÄ±:")
        category_counts = {}
        for listing in self.all_listings:
            key = f"{listing.get('category', 'bilinmeyen')}_{listing.get('transaction', 'bilinmeyen')}"
            category_counts[key] = category_counts.get(key, 0) + 1

        for cat, count in sorted(category_counts.items()):
            print(f"   {cat}: {count}")

        return self.all_listings


async def main():
    import argparse
    
    parser = argparse.ArgumentParser(description="Sahibinden Toplu Crawler")
    parser.add_argument("--headless", action="store_true", help="Headless mod (Ã¶nerilmez)")
    parser.add_argument("--categories", nargs="+", help="Sadece belirli kategoriler")
    parser.add_argument("--reset", action="store_true", help="Checkpoint sÄ±fÄ±rla")
    args = parser.parse_args()
    
    # Checkpoint sÄ±fÄ±rla
    if args.reset and os.path.exists(CHECKPOINT_FILE):
        os.remove(CHECKPOINT_FILE)
        print("ðŸ—‘ï¸ Checkpoint sÄ±fÄ±rlandÄ±")
    
    # Headless=False Ã¶nerilir (Cloudflare bypass iÃ§in)
    crawler = BatchCrawler(headless=args.headless)
    
    # Kategorileri belirle
    categories = args.categories if args.categories else None
    
    await crawler.run(categories)


if __name__ == "__main__":
    asyncio.run(main())
